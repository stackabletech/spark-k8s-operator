---
apiVersion: v1
data:
  properties.yaml: |
    version: 0.1.0
    spec:
      units:
        - unit: &unitPassword
            name: "password"
            regex: "^[a-zA-Z]\\w{5,20}$"
        - unit: &unitDirectory
            name: "directory"
            regex: "^(.*)/?([^/]+)$"
            examples:
              - "/tmp/xyz"
        - unit: &unitMemory
            name: "memory"
            regex: "(^\\p{N}+)(?:\\s*)((?:b|k|m|g|t|p|kb|mb|gb|tb|pb)\\b$)"
            examples:
              - "1024b"
              - "1024kb"
              - "500m"
              - "1g"

    properties:
      - property: &sparkWorkerCores
          propertyNames:
            - name: "SPARK_WORKER_CORES"
              kind:
                type: "file"
                file: "spark-env.sh"
          datatype:
             type: "integer"
          roles:
            - name: "slave"
              required: false
          asOfVersion: "0.6.2"
          description: "Total number of cores to allow Spark jobs to use on the machine (default: all available cores)"

      - property: &sparkWorkerMemory
          propertyNames:
            - name: "SPARK_WORKER_MEMORY"
              kind:
                type: "file"
                file: "spark-env.sh"
          datatype:
            type: "string"
            unit: *unitMemory
          roles:
            - name: "slave"
              required: false
          asOfVersion: "0.6.2"
          description: "Total amount of memory to allow Spark jobs to use on the machine, e.g. 1000M, 2G (default: total memory minus 1 GB); note that each job's individual memory is configured using SPARK_MEM."

      - property: &sparkDaemonMemory
          propertyNames:
            - name: "SPARK_DAEMON_MEMORY"
              kind:
                type: "file"
                file: "spark-env.sh"
          datatype:
            type: "string"
            unit: *unitMemory
          defaultValues:
            - fromVersion: "0.6.2"
              to_version: "1.4.1"
              value: "512m"
            - fromVersion: "1.5.0"
              value: "1g"
          roles:
            - name: "master"
              required: false
            - name: "slave"
              required: false
            - name: "history-server"
              required: false
          asOfVersion: "0.6.2"
          description: "Memory to allocate to the Spark master and worker daemons themselves (default: 512m/1g)"

      - property: &sparkEventLogEnabled
          propertyNames:
            - name: "spark.eventLog.enabled"
              kind:
                type: "file"
                file: "spark-defaults.conf"
          datatype:
            type: "bool"
          defaultValues:
            - fromVersion: "1.0.0"
              value: "false"
          recommendedValues:
            - fromVersion: "1.0.0"
              value: "true"
          roles:
            - name: "master"
              required: false
            - name: "slave"
              required: false
            - name: "history-server"
              required: false
          asOfVersion: "1.0.0"
          description: "Whether to log Spark events, useful for reconstructing the Web UI after the application has finished."

      - property: &sparkEventLogDir
          propertyNames:
            - name: "spark.eventLog.dir"
              kind:
                type: "file"
                file: "spark-defaults.conf"
          datatype:
            type: "string"
            unit: *unitDirectory
          defaultValues:
            - fromVersion: "1.0.0"
              value: "/tmp/spark-events"
          recommendedValues:
            - fromVersion: "1.0.0"
              value: "/stackable/log"
          roles:
            - name: "master"
              required: false
            - name: "slave"
              required: false
          asOfVersion: "1.0.0"
          expandsTo:
            - property: *sparkEventLogEnabled
              value: "true"
          description: "Base directory in which Spark events are logged, if spark.eventLog.enabled is true. Within this base directory, Spark creates a sub-directory for each application, and logs the events specific to the application in this directory. Users may want to set this to a unified location like an HDFS directory so history files can be read by the history server."

      - property: &sparkHistoryLogDirectory
          propertyNames:
            - name: "spark.history.fs.logDirectory"
              kind:
                type: "file"
                file: "spark-defaults.conf"
          datatype:
            type: "string"
            unit: *unitDirectory
          defaultValues:
            - fromVersion: "1.1.0"
              value: "/tmp/spark-events"
          recommendedValues:
            - fromVersion: "1.0.0"
              value: "/stackable/log"
          roles:
            - name: "history-server"
              required: true
          expandsTo:
            - property: *sparkEventLogEnabled
              value: "true"
          asOfVersion: "1.1.0"
          description: "For the filesystem history provider, the URL to the directory containing application event logs to load. This can be a local file://path, an HDFS path hdfs://namenode/shared/spark-logs or that of an alternative filesystem supported by the Hadoop APIs."

      - property: &sparkHistoryStorePath
          propertyNames:
            - name: "spark.history.store.path"
              kind:
                type: "file"
                file: "spark-defaults.conf"
          datatype:
            type: "string"
            unit: *unitDirectory
          roles:
            - name: "history-server"
              required: false
          asOfVersion: "2.3.0"
          description: "Local directory where to cache application history data. If set, the history server will store application data on disk instead of keeping it in memory. The data written to disk will be re-used in the event of a history server restart."

      - property: &sparkAuthenticate
          propertyNames:
            - name: "spark.authenticate"
              kind:
                type: "file"
                file: "spark-defaults.conf"
          datatype:
            type: "bool"
          defaultValues:
            - fromVersion: "1.0.0"
              value: "false"
          recommendedValues:
            - fromVersion: "1.0.0"
              value: "true"
          roles:
            - name: "master"
              required: false
            - name: "slave"
              required: false
            - name: "history-server"
              required: false
          asOfVersion: "1.0.0"
          description: "Whether Spark authenticates its internal connections."

      - property: &sparkAuthenticateSecret
          propertyNames:
            - name: "spark.authenticate.secret"
              kind:
                type: "file"
                file: "spark-defaults.conf"
          datatype:
            type: "string"
            unit: *unitPassword
          recommendedValues:
            - fromVersion: "1.0.0"
              value: "secret"
          roles:
            - name: "master"
              required: false
            - name: "slave"
              required: false
            - name: "history-server"
              required: false
          asOfVersion: "1.0.0"
          expandsTo:
            - property: *sparkAuthenticate
              value: "true"
          description: "The secret key used in the authentication. SPARK_AUTHENTICATE must be set to true."

      - property: &sparkPortMaxRetries
          propertyNames:
            - name: "spark.port.maxRetries"
              kind:
                type: "file"
                file: "spark-defaults.conf"
          datatype:
            type: "integer"
          defaultValues:
            - fromVersion: "1.1.1"
              value: "16"
          recommendedValues:
            - fromVersion: "1.1.1"
              value: "0"
          roles:
            - name: "master"
              required: true
            - name: "slave"
              required: true
            - name: "history-server"
              required: true
          asOfVersion: "1.1.1"
          description: "Maximum number of retries when binding to a port before giving up. When a port is given a specific value (non 0), each subsequent retry will increment the port used in the previous attempt by 1 before retrying. This essentially allows it to try a range of ports from the start port specified to port + maxRetries."

      - property: &sparkNoDaemonize
          propertyNames:
            - name: "SPARK_NO_DAEMONIZE"
              kind:
                type: "env"
          datatype:
            type: "bool"
          recommendedValues:
            - fromVersion: "2.0.0"
              value: "true"
          roles:
            - name: "master"
              required: true
            - name: "slave"
              required: true
            - name: "history-server"
              required: true
          asOfVersion: "2.0.0"
          description: "Run spark processes in foreground if true. Useful for systemd (default: false)"

      - property: &sparkConfDir
          propertyNames:
            - name: "SPARK_CONF_DIR"
              kind:
                type: "env"
          datatype:
            type: "string"
          recommendedValues:
            - fromVersion: "1.1.1"
              value: "/stackable/config"
          roles:
            - name: "master"
              required: true
            - name: "slave"
              required: true
            - name: "history-server"
              required: true
          asOfVersion: "1.1.1"
          description: "To specify a different configuration directory other than the default “SPARK_HOME/conf”, you can set SPARK_CONF_DIR. Spark will use the the configuration files (spark-defaults.conf, spark-env.sh, log4j.properties, etc) from this directory."
kind: ConfigMap
metadata:
  name: spark-k8s-operator-configmap
  labels:
    app.kubernetes.io/name: spark-k8s-operator
    app.kubernetes.io/instance: spark-k8s-operator
    app.kubernetes.io/version: "0.1.0"
