---
apiVersion: spark.stackable.tech/v1alpha1
kind: SparkApplication
metadata:
  name: spark-examples
spec:
  version: "1.0"
{% if lookup('env', 'VECTOR_AGGREGATOR') %}
  vectorAggregatorConfigMapName: vector-aggregator-discovery
{% endif %}
  sparkImage:
{% if test_scenario['values']['spark'].find(",") > 0 %}
    custom: "{{ test_scenario['values']['spark'].split(',')[1] }}"
    productVersion: "{{ test_scenario['values']['spark'].split(',')[0] }}"
{% else %}
    productVersion: "{{ test_scenario['values']['spark'] }}"
{% endif %}
    pullPolicy: IfNotPresent
  mode: cluster
  mainApplicationFile: "local:///stackable/spark/jobs/write-to-postgresql.py"
  job:
    logging:
      enableVectorAgent: {{ lookup('env', 'VECTOR_AGGREGATOR') | length > 0 }}
  driver:
    config:
      logging:
        enableVectorAgent: {{ lookup('env', 'VECTOR_AGGREGATOR') | length > 0 }}
      volumeMounts:
        - name: script
          mountPath: /stackable/spark/jobs
  executor:
    replicas: 1
    config:
      logging:
        enableVectorAgent: {{ lookup('env', 'VECTOR_AGGREGATOR') | length > 0 }}
      volumeMounts:
        - name: script
          mountPath: /stackable/spark/jobs
  deps:
    packages:
      - org.postgresql:postgresql:42.6.0
  volumes:
    - name: script
      configMap:
        name: write-to-postgresql
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: write-to-postgresql
data:
  write-to-postgresql.py: |
    from pyspark.sql import SparkSession
    from pyspark.sql.types import *

    spark = SparkSession.builder.appName("write-to-postgresql").getOrCreate()

    df = spark.createDataFrame([1,2,3], IntegerType())

    # Specifying create table column data types on write
    df.write \
    .option("createTableColumnTypes", "value INTEGER") \
    .jdbc("jdbc:postgresql://spark-postgresql/spark", "sparktest",
          properties={"user": "spark", "password": "spark"})
