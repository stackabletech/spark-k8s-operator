---
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-connect-log-config
data:
  log4j2.properties: |-
    appenders = CONSOLE

    appender.CONSOLE.type = Console
    appender.CONSOLE.name = CONSOLE
    appender.CONSOLE.target = SYSTEM_ERR
    appender.CONSOLE.layout.type = PatternLayout
    appender.CONSOLE.layout.pattern = %d{ISO8601} %p [%t] %c - %m%n
    appender.CONSOLE.filter.threshold.type = ThresholdFilter
    appender.CONSOLE.filter.threshold.level = DEBUG

    rootLogger.level=DEBUG
    rootLogger.appenderRefs = CONSOLE
    rootLogger.appenderRef.CONSOLE.ref = CONSOLE
---
apiVersion: spark.stackable.tech/v1alpha1
kind: SparkConnectServer
metadata:
  name: spark-connect
spec:
  image:
{% if test_scenario['values']['spark-connect'].find(",") > 0 %}
    custom: "{{ test_scenario['values']['spark-connect'].split(',')[1] }}"
    productVersion: "{{ test_scenario['values']['spark-connect'].split(',')[0] }}"
{% else %}
    productVersion: "{{ test_scenario['values']['spark-connect'] }}"
{% endif %}
    pullPolicy: IfNotPresent
{% if lookup('env', 'VECTOR_AGGREGATOR') %}
  vectorAggregatorConfigMapName: vector-aggregator-discovery
{% endif %}
  args:
    # These are unfortunatly required to make the S3A connector work with MinIO
    # I had expected the clients to be able to set these, but that is not the case.
    - --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
    - --conf spark.hadoop.fs.s3a.path.style.access=true
    - --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000
    - --conf spark.hadoop.fs.s3a.region=us-east-1
  server:
    podOverrides:
      spec:
        containers:
          - name: spark
            env:
              - name: AWS_ACCESS_KEY_ID
                valueFrom:
                  secretKeyRef:
                    name: s3-credentials
                    key: accessKey
              - name: AWS_SECRET_ACCESS_KEY
                valueFrom:
                  secretKeyRef:
                    name: s3-credentials
                    key: secretKey

    jvmArgumentOverrides:
      add:
        - -Dmy.custom.jvm.arg=customValue
    config:
      listenerClass: external-unstable
      logging:
        enableVectorAgent: {{ lookup('env', 'VECTOR_AGGREGATOR') | length > 0 }}
        containers:
          spark:
            custom:
              configMap: spark-connect-log-config
    configOverrides:
      spark-defaults.conf:
        spark.jars.ivy: /tmp/ivy2
  executor:
    configOverrides:
      spark-defaults.conf:
        spark.executor.instances: "1"
        spark.executor.memoryOverhead: "1m"
    config:
      logging:
        enableVectorAgent: {{ lookup('env', 'VECTOR_AGGREGATOR') | length > 0 }}
        containers:
          spark:
            custom:
              configMap: spark-connect-log-config
