---
apiVersion: spark.stackable.tech/v1alpha1
kind: SparkApplication
metadata:
  name: pyspark-iceberg
spec:
{% if lookup('env', 'VECTOR_AGGREGATOR') %}
  vectorAggregatorConfigMapName: vector-aggregator-discovery
{% endif %}
  sparkImage:
{% if test_scenario['values']['spark'].find(",") > 0 %}
    custom: "{{ test_scenario['values']['spark'].split(',')[1] }}"
    productVersion: "{{ test_scenario['values']['spark'].split(',')[0] }}"
{% else %}
    productVersion: "{{ test_scenario['values']['spark'] }}"
{% endif %}
    pullPolicy: IfNotPresent
  mode: cluster
  mainApplicationFile: "local:///stackable/spark/jobs/write-to-iceberg.py"
  sparkConf:
    spark.sql.extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
    spark.sql.catalog.spark_catalog: org.apache.iceberg.spark.SparkSessionCatalog
    spark.sql.catalog.spark_catalog.type: hive
    spark.sql.catalog.local: org.apache.iceberg.spark.SparkCatalog
    spark.sql.catalog.local.type: hadoop
    spark.sql.catalog.local.warehouse: /tmp/warehouse
  driver:
    config:
      logging:
        enableVectorAgent: {{ lookup('env', 'VECTOR_AGGREGATOR') | length > 0 }}
      volumeMounts:
        - name: script
          mountPath: /stackable/spark/jobs
  executor:
    replicas: 1
    config:
      logging:
        enableVectorAgent: {{ lookup('env', 'VECTOR_AGGREGATOR') | length > 0 }}
      volumeMounts:
        - name: script
          mountPath: /stackable/spark/jobs
  deps:
    packages:
      #
      # The iceberg runtime contains the spark and scala versions in the form :
      #
      #   <spark major>.<spark minor>_<scala major>.<scala minor>
      #
      # We extract the spark parts from the test scenario value.
      #
      - org.apache.iceberg:iceberg-spark-runtime-{{ ".".join(test_scenario['values']['spark'].split('.')[:2]) }}_2.12:1.8.1
  volumes:
    - name: script
      configMap:
        name: write-to-iceberg
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: write-to-iceberg
data:
  write-to-iceberg.py: |
    from pyspark.sql import SparkSession
    from pyspark.sql.types import *

    spark = SparkSession.builder.appName("write-to-iceberg").getOrCreate()

    schema = StructType([
      StructField("id", LongType(), True),
      StructField("data", StringType(), True)
    ])


    # create table
    df = spark.createDataFrame([], schema)
    df.writeTo("local.db.table").create()

    # append to table
    data = [
        (1,"one"),
        (2,"two"),
        (3,"three"),
        (4,"four")
      ]

    df = spark.createDataFrame(data, schema)
    df.writeTo("local.db.table").append()
