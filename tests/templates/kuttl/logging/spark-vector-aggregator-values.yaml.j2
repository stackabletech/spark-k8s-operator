---
role: Aggregator
service:
  ports:
  - name: api
    port: 8686
    protocol: TCP
    targetPort: 8686
  - name: vector
    port: 6123
    protocol: TCP
    targetPort: 6000
customConfig:
  api:
    address: 0.0.0.0:8686
    enabled: true
  sources:
    vector:
      address: 0.0.0.0:6000
      type: vector
      version: "2"
  transforms:

    # SparkHistoryServer spark-history

    filteredSparkHistoryAutomaticLogConfigSparkHistory:
      type: filter
      inputs: [vector]
      condition: >-
        .pod == "spark-history-node-automatic-log-config-0" &&
        .container == "spark-history"
    filteredSparkHistoryAutomaticLogConfigVector:
      type: filter
      inputs: [vector]
      condition: >-
        .pod == "spark-history-node-automatic-log-config-0" &&
        .container == "vector"
    filteredSparkHistoryCustomLogConfigSparkHistory:
      type: filter
      inputs: [vector]
      condition: >-
        .pod == "spark-history-node-custom-log-config-0" &&
        .container == "spark-history"
    filteredSparkHistoryCustomLogConfigVector:
      type: filter
      inputs: [vector]
      condition: >-
        .pod == "spark-history-node-custom-log-config-0" &&
        .container == "vector"

    # SparkApplication spark-automatic-log-config

    filteredSparkAutomaticLogConfigDriverSpark:
      type: filter
      inputs: [vector]
      condition: >-
        .cluster == "spark-automatic-log-config" &&
        ends_with(string!(.pod), "-driver") &&
        .container == "spark"
    filteredSparkAutomaticLogConfigDriverJob:
      type: filter
      inputs: [vector]
      condition: >-
        .cluster == "spark-automatic-log-config" &&
        ends_with(string!(.pod), "-driver") &&
        .container == "job"
    filteredSparkAutomaticLogConfigDriverVector:
      type: filter
      inputs: [vector]
      condition: >-
        .cluster == "spark-automatic-log-config" &&
        ends_with(string!(.pod), "-driver") &&
        .container == "vector"
    filteredSparkAutomaticLogConfigExecutorSpark:
      type: filter
      inputs: [vector]
      condition: >-
        .cluster == "spark-automatic-log-config" &&
        ends_with(string!(.pod), "-exec-1") &&
        .container == "spark"
    filteredSparkAutomaticLogConfigExecutorJob:
      type: filter
      inputs: [vector]
      condition: >-
        .cluster == "spark-automatic-log-config" &&
        ends_with(string!(.pod), "-exec-1") &&
        .container == "job"
    filteredSparkAutomaticLogConfigExecutorVector:
      type: filter
      inputs: [vector]
      condition: >-
        .cluster == "spark-automatic-log-config" &&
        ends_with(string!(.pod), "-exec-1") &&
        .container == "vector"

    # SparkApplication spark-custom-log-config

    filteredSparkCustomLogConfigDriverSpark:
      type: filter
      inputs: [vector]
      condition: >-
        .cluster == "spark-custom-log-config" &&
        ends_with(string!(.pod), "-driver") &&
        .container == "spark"
    filteredSparkCustomLogConfigDriverJob:
      type: filter
      inputs: [vector]
      condition: >-
        .cluster == "spark-custom-log-config" &&
        ends_with(string!(.pod), "-driver") &&
        .container == "job"
    filteredSparkCustomLogConfigDriverVector:
      type: filter
      inputs: [vector]
      condition: >-
        .cluster == "spark-custom-log-config" &&
        ends_with(string!(.pod), "-driver") &&
        .container == "vector"
    filteredSparkCustomLogConfigExecutorSpark:
      type: filter
      inputs: [vector]
      condition: >-
        .cluster == "spark-custom-log-config" &&
        ends_with(string!(.pod), "-exec-1") &&
        .container == "spark"
    filteredSparkCustomLogConfigExecutorJob:
      type: filter
      inputs: [vector]
      condition: >-
        .cluster == "spark-custom-log-config" &&
        ends_with(string!(.pod), "-exec-1") &&
        .container == "job"
    filteredSparkCustomLogConfigExecutorVector:
      type: filter
      inputs: [vector]
      condition: >-
        .cluster == "spark-custom-log-config" &&
        ends_with(string!(.pod), "-exec-1") &&
        .container == "vector"

    # SparkApplication pyspark-automatic-log-config

    filteredPysparkAutomaticLogConfigDriverSpark:
      type: filter
      inputs: [vector]
      condition: >-
        .cluster == "pyspark-automatic-log-config" &&
        ends_with(string!(.pod), "-driver") &&
        .container == "spark"
    filteredPysparkAutomaticLogConfigDriverRequirements:
      type: filter
      inputs: [vector]
      condition: >-
        .cluster == "pyspark-automatic-log-config" &&
        ends_with(string!(.pod), "-driver") &&
        .container == "requirements"
    filteredPysparkAutomaticLogConfigDriverVector:
      type: filter
      inputs: [vector]
      condition: >-
        .cluster == "pyspark-automatic-log-config" &&
        ends_with(string!(.pod), "-driver") &&
        .container == "vector"
    filteredPysparkAutomaticLogConfigExecutorSpark:
      type: filter
      inputs: [vector]
      condition: >-
        .cluster == "pyspark-automatic-log-config" &&
        ends_with(string!(.pod), "-exec-1") &&
        .container == "spark"
    filteredPysparkAutomaticLogConfigExecutorRequirements:
      type: filter
      inputs: [vector]
      condition: >-
        .cluster == "pyspark-automatic-log-config" &&
        ends_with(string!(.pod), "-exec-1") &&
        .container == "requirements"
    filteredPysparkAutomaticLogConfigExecutorVector:
      type: filter
      inputs: [vector]
      condition: >-
        .cluster == "pyspark-automatic-log-config" &&
        ends_with(string!(.pod), "-exec-1") &&
        .container == "vector"

    # SparkApplication pyspark-custom-log-config

    filteredPysparkCustomLogConfigDriverSpark:
      type: filter
      inputs: [vector]
      condition: >-
        .cluster == "pyspark-custom-log-config" &&
        ends_with(string!(.pod), "-driver") &&
        .container == "spark"
    filteredPysparkCustomLogConfigDriverRequirements:
      type: filter
      inputs: [vector]
      condition: >-
        .cluster == "pyspark-custom-log-config" &&
        ends_with(string!(.pod), "-driver") &&
        .container == "requirements"
    filteredPysparkCustomLogConfigDriverVector:
      type: filter
      inputs: [vector]
      condition: >-
        .cluster == "pyspark-custom-log-config" &&
        ends_with(string!(.pod), "-driver") &&
        .container == "vector"
    filteredPysparkCustomLogConfigExecutorSpark:
      type: filter
      inputs: [vector]
      condition: >-
        .cluster == "pyspark-custom-log-config" &&
        ends_with(string!(.pod), "-exec-1") &&
        .container == "spark"
    filteredPysparkCustomLogConfigExecutorRequirements:
      type: filter
      inputs: [vector]
      condition: >-
        .cluster == "pyspark-custom-log-config" &&
        ends_with(string!(.pod), "-exec-1") &&
        .container == "requirements"
    filteredPysparkCustomLogConfigExecutorVector:
      type: filter
      inputs: [vector]
      condition: >-
        .cluster == "pyspark-custom-log-config" &&
        ends_with(string!(.pod), "-exec-1") &&
        .container == "vector"

    # Invalid events

    filteredInvalidEvents:
      type: filter
      inputs: [vector]
      condition: |-
        .timestamp == from_unix_timestamp!(0) ||
        is_null(.level) ||
        is_null(.logger) ||
        is_null(.message)
  sinks:
    out:
      inputs: [filtered*]
{% if lookup('env', 'VECTOR_AGGREGATOR') %}
      type: vector
      address: {{ lookup('env', 'VECTOR_AGGREGATOR') }}
      buffer:
        # Avoid back pressure from VECTOR_AGGREGATOR. The test should
        # not fail if the aggregator is not available.
        when_full: drop_newest
{% else %}
      type: blackhole
{% endif %}
