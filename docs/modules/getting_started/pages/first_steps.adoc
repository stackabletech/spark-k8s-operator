= First steps

Once you have followed the steps in the xref:installation.adoc[] section to install the Operator and its dependencies, you will now create a Spark job. Afterwards you can <<_verify_that_it_works, verify that it works>> by looking at the logs from the driver pod.

=== Airflow

An Airflow cluster is made of up three components:

- `webserver`: this provides the main UI for user-interaction
- `workers`: the nodes over which the job workload will be distributed by the scheduler
- `scheduler`: responsible for triggering jobs and persisting their metadata to the backend database

Create a file named `pyspark-pi.yaml` with the following contents:

[source,yaml]
----
include::example$code/pyspark-pi.yaml[]
----

And apply it:

----
include::example$code/getting_started.sh[tag=install-sparkapp]
----

Where:

- `metadata.name` contains the name of the SparkApplication
- `spec.version`: the current version is "1.0"
- `spec.sparkImage`: the docker image that will be used by job, driver and executor pods. This can be provided by the user.
- `spec.mode`: only `cluster` is currently supported
- `spec.mainApplicationFile`: the artifact (Java, Scala or Python) that forms the basis of the Spark job.
- `spec.driver`: driver-specific settings.
- `spec.executor`: executor-specific settings.


NOTE: If using Stackable image versions, please note that the version you need to specify for `spec.version` is not only the version of Spark which you want to roll out, but has to be amended with a Stackable version as shown. This Stackable version is the version of the underlying container image which is used to execute the processes. For a list of available versions please check our
https://repo.stackable.tech/#browse/browse:docker:v2%2Fstackable%spark-k8s%2Ftags[image registry].
It should generally be safe to simply use the latest image version that is available.

This will create the SparkApplication that in turn creates the Spark job.
