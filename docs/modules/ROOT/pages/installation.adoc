= Installation

There are three ways to run the Spark Operator:

1. Helm managed Docker container deployment on Kubernetes

2. Build from source

== Helm

Helm allows you to download and deploy Stackable operators on Kubernetes and is by far the easiest
installation method. First ensure that you have installed the Stackable Operators Helm repository:

[source,bash]
----
helm repo add stackable https://repo.stackable.tech/repository/helm-stable/
----

Then install the Stackable Operator for Apache Spark
[source,bash]
----
helm install spark-k8s-operator stackable/spark-k8s-operator
----

Helm will deploy the operator in a Kubernetes container and apply the CRDs for the Apache Spark
service. You are now ready to deploy Apache Spark in Kubernetes.

== Building the operator from source

This operator is written in Rust and is developed against a recent stable Rust release.

[source,bash]
----
cargo run -- crd | kubectl apply -f -
cargo run -- run
----

To run it from your local machine - usually for development purposes - you need to create a `ClusterRoleBinding` :

[source,bash]
----
kubectl create clusterrolebinding spark-role --clusterrole=spark-driver-edit-role  --serviceaccount=default:default
----

== Additional/Optional components

The above describes the installation of the operator alone and is sufficient for spark jobs that do not require any external dependencies. In practice, this is often not the case and spark- and/or job-dependencies will be required. These can be made available in different ways - e.g. by including them in the spark images used by `spark-submit`, reading them external repositories or by using local external storage such as Kuberentes persistent volumes. This last option requires the following:

== Job Dependencies

include::job_dependencies.adoc[]

== Role-based Access Control (RBAC)

include::rbac.adoc[]

== Examples

The examples provided with the operator code show different ways of combining these elements.