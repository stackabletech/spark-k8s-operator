= Usage

== Create an Apache Spark job

If you followed the installation instructions, you should now have a Stackable Operator for Apache Spark up and running and you are ready to create your first Apache Spark kubernetes cluster.

The example below creates a job running on Apache Spark 3.2.1, using the spark-on-kubernetes paradigm described in the spark documentation.

    cat <<EOF | kubectl apply -f -
    apiVersion: spark.stackable.tech/v1alpha1
    kind: SparkApplication
    metadata:
      name: spark-clustermode-001
    spec:
      version: 3.2.1-hadoop3.2
      mode: cluster
      mainClass: org.apache.spark.examples.SparkPi
      mainApplicationFile: local:///stackable/spark/examples/jars/spark-examples_2.12-3.2.1.jar
      image: 3.2.1-hadoop3.2
      driver:
        cores: 1
        coreLimit: "1200m"
        memory: "512m"
      executor:
        cores: 1
        instances: 3
        memory: "512m"
      config:
        enableMonitoring: true
    EOF


== Configuration & Environment Overrides

The cluster definition also supports overriding configuration properties and environment variables, either per role or per role group, where the more specific override (role group) has precedence over the less specific one (role).

=== Configuration Properties

All override property values must be strings. The properties will be passed on without any escaping or formatting.

=== Environment Variables

Environment variables can be (over)written by adding them to `spark-env.sh` file as described above or by using the `envOverrides` property.

=== Development

export KUBERNETES_SERVICE_PORT_HTTPS=443
export KUBERNETES_SERVICE_HOST=10.96.0.1
kubectl create clusterrolebinding spark-role --clusterrole=edit  --serviceaccount=default:default
